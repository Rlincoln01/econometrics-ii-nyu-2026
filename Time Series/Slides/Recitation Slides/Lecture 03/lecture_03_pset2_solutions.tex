\documentclass[aspectratio=169,11pt]{beamer}

% Theme and color setup
\usetheme{default}
\usecolortheme{default}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Custom colors (MIT-inspired)
\definecolor{mitred}{RGB}{163,31,52}
\definecolor{darkblue}{RGB}{0,40,85}
\definecolor{lightgray}{RGB}{245,245,245}
\definecolor{darkgray}{RGB}{64,64,64}

% Set main colors
\setbeamercolor{frametitle}{fg=white,bg=darkblue}
\setbeamercolor{title}{fg=darkblue}
\setbeamercolor{author}{fg=black}
\setbeamercolor{institute}{fg=black}
\setbeamercolor{date}{fg=black}
\setbeamercolor{structure}{fg=darkblue}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{itemize item}{fg=mitred}
\setbeamercolor{itemize subitem}{fg=darkblue}
\setbeamercolor{enumerate item}{fg=darkblue}

% Frame title band
\newlength{\titlebandht}
\setlength{\titlebandht}{4ex}

\setbeamertemplate{frametitle}{
  \nointerlineskip
  \begin{beamercolorbox}[wd=\paperwidth,ht=\titlebandht,dp=1.1ex,leftskip=1.2em,rightskip=1.2em]{frametitle}
    \usebeamerfont{frametitle}\insertframetitle
    \ifx\insertframesubtitle\@empty\relax\else\\[-0.2ex]
      \usebeamerfont{framesubtitle}\insertframesubtitle
    \fi
  \end{beamercolorbox}
  \vspace{0.6em}
}

% Font settings
\usefonttheme{professionalfonts}
\usefonttheme{serif}

% Footline with page numbers
\setbeamertemplate{footline}{
    \hfill\insertframenumber\hspace{2em}\vspace{0.5em}
}

% Packages
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{bbm}
\usepackage{hyperref}

% Custom commands for math
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Corr}{\text{Corr}}
\newcommand{\Span}{\text{span}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\vbf}{\mathbf{v}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\proj}{proj}

% Theorem environments
\setbeamertemplate{theorems}[numbered]
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\theoremstyle{definition}

% Block colors
\setbeamercolor{block title}{bg=lightgray,fg=darkblue}
\setbeamercolor{block body}{bg=lightgray!50}

% Alert block colors
\setbeamercolor{block title alerted}{bg=mitred!15,fg=mitred}
\setbeamercolor{block body alerted}{bg=mitred!5}

% Title information
\title{Problem Set 2: Solutions}
\subtitle{State Space, Trends, and Policy Regime Change}
\author{Rafael Lincoln}
\institute{Econometrics II -- NYU PhD Program}
\date{Spring 2026}

\begin{document}

% ============================================================================
% TITLE SLIDE
% ============================================================================
\begin{frame}[plain]
    \titlepage
\end{frame}

% ============================================================================
% OUTLINE
% ============================================================================
\begin{frame}{Today's Roadmap}
    \textbf{Problem Set 2 Solutions}
    \begin{enumerate}
        \item \textbf{Problem 1: Long-Run Risk (Bansal--Yaron)}
        \begin{itemize}
            \item VAR representation, stationarity, Kalman filter
        \end{itemize}
        \vspace{0.3em}
        \item \textbf{Problem 2: Beveridge--Nelson Trend}
        \begin{itemize}
            \item VAR-based trend extraction, random walk property
        \end{itemize}
        \vspace{0.3em}
        \item \textbf{Problem 3: Clarida--Gal\'{\i}--Gertler NK Model}
        \begin{itemize}
            \item AR(1) derivation, LR/Wald/LM tests, Lucas critique
        \end{itemize}
        \vspace{0.3em}
        \item \textbf{Problem 4: Kalman Filter with Correlated Innovations}
        \vspace{0.3em}
        \item \textbf{Problem 5: ARMA(2,2) to State Space}
    \end{enumerate}
\end{frame}

% ============================================================================
% PROBLEM 1: LONG-RUN RISK
% ============================================================================
\begin{frame}[plain]
    \begin{center}
        {\Large \textbf{Problem 1: Long-Run Risk}}\\[1em]
        {\large Bansal--Yaron (2004) Model}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1: Setup}
    \textbf{Model} (Bansal--Yaron long-run risk):
    \begin{align*}
        x_t &= \rho x_{t-1} + \sigma_\varepsilon \varepsilon_t \\
        g_t &= x_t + \sigma_\eta \eta_t
    \end{align*}
    where $(\varepsilon_t, \eta_t)' \sim \text{i.i.d. } \mathcal{N}(0, I_2)$.
    
    \vspace{0.5em}
    
    \textbf{Parameters}: $\theta = (\rho, \sigma_\varepsilon, \sigma_\eta)'$
    
    \vspace{0.5em}
    
    \textbf{Interpretation}:
    \begin{itemize}
        \item $x_t$: latent expected growth rate (``long-run risk'')
        \item $g_t$: observed consumption/dividend growth
        \item The latent $x_t$ creates persistent predictability in growth
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(a): VAR Representation}
    \small
    \textbf{Question}: \textit{Derive a VAR representation for $(x_t, g_t)'$. What condition on $\theta$ is necessary for covariance stationarity?}
    
    \textbf{Solution}: Stack the equations:
    \[
    \underbrace{\begin{pmatrix} 1 & 0 \\ -1 & 1 \end{pmatrix}}_{B}
    \begin{pmatrix} x_t \\ g_t \end{pmatrix}
    =
    \underbrace{\begin{pmatrix} \rho & 0 \\ 0 & 0 \end{pmatrix}}_{A}
    \begin{pmatrix} x_{t-1} \\ g_{t-1} \end{pmatrix}
    +
    \underbrace{\begin{pmatrix} \sigma_\varepsilon & 0 \\ 0 & \sigma_\eta \end{pmatrix}}_{C}
    \begin{pmatrix} \varepsilon_t \\ \eta_t \end{pmatrix}
    \]
    
    Premultiply by $B^{-1} = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}$:
    \[
    \begin{pmatrix} x_t \\ g_t \end{pmatrix}
    =
    \begin{pmatrix} \rho & 0 \\ \rho & 0 \end{pmatrix}
    \begin{pmatrix} x_{t-1} \\ g_{t-1} \end{pmatrix}
    +
    \begin{pmatrix} \sigma_\varepsilon & 0 \\ \sigma_\varepsilon & \sigma_\eta \end{pmatrix}
    \begin{pmatrix} \varepsilon_t \\ \eta_t \end{pmatrix}
    \]
    
    \begin{alertblock}{Stationarity Condition}
        Eigenvalues of $B^{-1}A$ are $\{\rho, 0\}$. \textbf{Stationarity requires $|\rho| < 1$}.
    \end{alertblock}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(a): Stationarity --- Two Perspectives}
    \textbf{Why $|\rho| < 1$?} Two equivalent ways to see this:
    
    \vspace{0.3em}
    
    \textbf{1. Characteristic polynomial approach}:
    \begin{itemize}
        \item Stationarity requires all roots of the characteristic polynomial $\det(I - \Phi z) = 0$ to lie \textbf{outside} the unit circle (equivalently, eigenvalues of $\Phi$ inside unit circle)
        \item For our VAR: $\det\begin{pmatrix} 1 - \rho z & 0 \\ -\rho z & 1 \end{pmatrix} = (1-\rho z) \cdot 1 = 0$
        \item Root: $z = 1/\rho$. For $|z| > 1$, need $|\rho| < 1$
    \end{itemize}
    
    \vspace{0.3em}
    
    \textbf{2. Economic intuition}:
    \begin{itemize}
        \item $x_t = \rho x_{t-1} + \sigma_\varepsilon \varepsilon_t$ is AR(1) --- stationary iff $|\rho| < 1$
        \item $g_t = x_t + \sigma_\eta \eta_t$ is just $x_t$ plus i.i.d.\ noise
        \item If $x_t$ is stationary, then $g_t$ must also be stationary
        \item The ``extra noise'' $\eta_t$ doesn't affect persistence, only variance
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(b): Likelihood for Dependent Data --- Review}
    \small
    \textbf{Question}: \textit{Assuming stationarity and that $x_t$ and $g_t$ are both observable, explain how you would estimate $\theta$ by maximum likelihood.}
    
    \vspace{0.3em}
    
    \textbf{What we need}:
    \begin{itemize}
        \item For \textbf{dependent data}, the joint density is not a product of i.i.d.\ marginals.
        \item We use the \textbf{decomposition of the log-likelihood} (with $\F^Y_{t-1} = \sigma(Y_0, \ldots, Y_{t-1})$, process adapted to $Y$):
    \end{itemize}
    \[
    \log f(Y_0, Y_1, \ldots, Y_T) \;=\; \log f(Y_0) + \sum_{t=1}^{T} \log f(Y_t \mid \F^Y_{t-1})
    \]
    
    \textbf{VAR(1) case}:
    \begin{itemize}
        \item $Y_t = \Phi Y_{t-1} + u_t$, \quad $u_t \sim \mathcal{N}(0, \Sigma_u)$.
        \item By \textbf{Markov property}: $Y_t \mid \F^Y_{t-1}$ depends only on $Y_{t-1}$, and is Gaussian:
    \end{itemize}
    \[
    Y_t \mid \F^Y_{t-1} \;\sim\; \mathcal{N}(\Phi Y_{t-1},\, \Sigma_u)
    \]
    
    \begin{itemize}
        \item Write the log-likelihood (including $\log f(Y_0)$) and maximize it $\Rightarrow$ next slide.
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(b): Solution --- MLE = OLS}
    \small
    \textbf{From the decomposition}: Let $Y_t = (x_t, g_t)'$. The reduced-form VAR is $Y_t = \Phi Y_{t-1} + u_t$, $u_t \sim \mathcal{N}(0, \Sigma_u)$.
    
    \textbf{Log-likelihood}: $\ell_T(\theta) = \log f(Y_0) + \sum_{t=1}^{T} \log f(Y_t \mid \F^Y_{t-1})$. For the Gaussian VAR, the conditional terms give:
    \[
    \ell_T(\theta) = \log f(Y_0) - \frac{T}{2}\log|\Sigma_u| - \frac{1}{2}\sum_{t=1}^{T} (Y_t - \Phi Y_{t-1})'\Sigma_u^{-1}(Y_t - \Phi Y_{t-1})
    \]
    
    \vspace{0.2em}
    
    Maximizing w.r.t.\ $\Phi$: the term that depends on $\Phi$ is the sum of squared (generalized) residuals. Under Gaussianity, the \textbf{MLE for $\Phi$ is OLS} equation-by-equation:
    \[
    \hat{\Phi} = \left(\sum_{t=1}^T Y_t Y_{t-1}'\right)\left(\sum_{t=1}^T Y_{t-1}Y_{t-1}'\right)^{-1}
    \]
    
    Then back out $(\rho, \sigma_\varepsilon, \sigma_\eta)$ from $\hat{\Phi}$ and $\hat{\Sigma}_u$.
\end{frame}

% ============================================================================
% PROBLEM 1(c): REVIEW --- State-space and likelihood (following supplied slides)
% ============================================================================
\begin{frame}{Problem 1(c): Review --- From state-space model to likelihood}
    \small
    \textbf{Question}: \textit{Now assume that $g_t$ is observable but $x_t$ is not. Assuming stationarity, explain how you would estimate $\theta$ by maximum likelihood.}
    
    \vspace{0.4em}
    
    \textbf{Review}:
    \begin{itemize}
        \item Besides this problem, the same ideas matter in a \textbf{broader context}: \textbf{estimation of structural models}.
        \item Classical ML and \textbf{Bayesian} estimation both rely on likelihood evaluation.
        \item Why useful? We use it when estimating \textbf{structural models} (e.g.\ business-cycle models with unobserved states).
    \end{itemize}
    
    \vspace{0.2em}
    
    \textbf{From state-space model to likelihood}: A model is a parameter vector $\psi$ giving rise to a state-space system for observables $y_t$:
    \begin{itemize}
        \item \textbf{Observation equation}: $y_t = \Psi(s_t; \psi) + u_t$, \quad $u_t \sim F_u(\cdot; \psi)$
        \item \textbf{State transition}: $s_t = \Phi(s_{t-1}, \varepsilon_t; \psi)$, \quad $\varepsilon_t \sim F_\varepsilon(\cdot; \psi)$
    \end{itemize}
    
    \vspace{0.2em}
    
    \textbf{Q}: What is the likelihood of $y_{1:T}$ given $\psi$? \textbf{Standard approach: filtering.}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(c): Review --- Likelihood factorization and evaluation}
    \footnotesize
    \textbf{Likelihood factorization} ($\F^Y_{t-1} = \sigma(y_0, \ldots, y_{t-1})$):
    \[
    p(y_{1:T} \mid \psi) \;=\; \prod_{t=1}^{T} p(y_t \mid \F^Y_{t-1}, \psi)
    \]
    
    \textbf{0.} Initial: $p(s_0 \mid y_{1:0})$ (e.g.\ stationary).
    
    \textbf{1. Forecasting} (predict $s_t$, then $y_t$, given past):
    \begin{itemize}
        \item \textbf{(a) Transition equation:}
        \[
        p(s_t \mid y_{1:t-1}) \;=\; \int p(s_t \mid s_{t-1}, y_{1:t-1})\, p(s_{t-1} \mid y_{1:t-1})\, ds_{t-1}
        \]
        \item \textbf{(b) Measurement equation:}
        \[
        p(y_t \mid y_{1:t-1}) \;=\; \int p(y_t \mid s_t, y_{1:t-1})\, p(s_t \mid y_{1:t-1})\, ds_t
        \]
    \end{itemize}
    
    \textbf{2. Updating} (Bayes):
    \[
    p(s_t \mid y_{1:t}) \;=\; \frac{p(y_t \mid s_t, y_{1:t-1})\, p(s_t \mid y_{1:t-1})}{p(y_t \mid y_{1:t-1})}
    \]
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(c): Review --- Kalman filter (Gaussian case)}
    \small
    \textbf{When the model is linear and shocks are Gaussian}, we get the \textbf{Kalman filter}---and everything becomes easy.
    
    \vspace{0.3em}
    
    \textbf{Likelihood}: Chain rule $p(y^T) = p(y_T \mid y^{T-1}) \cdots p(y_2 \mid y_1)\, p(y_1)$, with $y^t = (y_t, \ldots, y_1)$. Under normality:
    \[
    y_t \mid \F^Y_{t-1} \;\sim\; \mathcal{N}(\hat{y}_{t|t-1},\, V_{t|t-1})
    \]
    
    \vspace{0.3em}
    
    \textbf{Definitions} (linear model $y_t = b_t + Z_t s_t + u_t$, $s_t = A_t s_{t-1} + B_t \varepsilon_t$):
    \begin{align*}
    \hat{y}_{t|s} &\equiv \E[y_t \mid y^s] \;=\; b_t + Z_t \hat{s}_{t|s} \\
    V_{t|s} &\equiv \Var(y_t \mid y^s) \;=\; Z_t P_{t|s} Z_t' + H_t \\
    \hat{s}_{t|s} &\equiv \E[s_t \mid y^s], \qquad P_{t|s} \equiv \Var(s_t \mid y^s)
    \end{align*}
    with $H_t = \Var(u_t)$, $Q_t = \Var(\varepsilon_t)$.
    
    \vspace{0.2em}
    
    If we have $\hat{s}_{t|t-1}$ and $P_{t|t-1}$, we can compute the \textbf{log-likelihood}. These objects are computed recursively from $\hat{s}_{0|0}$, $P_{0|0}$.
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(c): Review --- The Kalman filter}
    \footnotesize
    \begin{itemize}
        \item If we can compute $\hat{s}_{t|t-1}$ and $P_{t|t-1}$, we can compute the log likelihood.
        \begin{itemize}
            \item[-] We compute these \textbf{recursively}, starting from $\hat{s}_{0|0}$ and $P_{0|0}$.
        \end{itemize}
    \end{itemize}
    
    \textbf{Forecasting step.} Use $\hat{s}_{t-1|t-1}$, $P_{t-1|t-1}$ and the transition equation to create forecasts:
    \[
    \hat{s}_{t|t-1} = A_t \hat{s}_{t-1|t-1}, \qquad P_{t|t-1} = A_t P_{t-1|t-1} A_t' + B_t Q_t B_t' \quad (Q_t = \Var(\varepsilon_t))
    \]
    Observation forecast: $\hat{y}_{t|t-1} = b_t + Z_t \hat{s}_{t|t-1}$, \quad $V_{t|t-1} = Z_t P_{t|t-1} Z_t' + H_t$.
    
    \textbf{Updating step.} Update $\hat{s}_{t|t-1}$ and $P_{t|t-1}$ to condition on $y_t$.
    \begin{itemize}
        \item[-] Forecast error: $\tilde{y}_t = y_t - \hat{y}_{t|t-1}$.
        \item[-] Kalman gain: $K_t = P_{t|t-1} Z_t' V_{t|t-1}^{-1}$.
        \item[-] Update $\hat{s}_{t|t} = \hat{s}_{t|t-1} + K_t \tilde{y}_t$.
        \item[-] Update $P_{t|t} = (I - K_t Z_t) P_{t|t-1}$.
    \end{itemize}
    
    \textbf{Log-likelihood} (scalar): $\ell_T(\psi) = -\frac{1}{2}\sum_{t=1}^T \left[\log V_{t|t-1} + \tilde{y}_t^2/V_{t|t-1}\right]$ + const.
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(c): MLE When $x_t$ Latent --- State Space}
    \small
    \textbf{Question}: \textit{Now assume that $g_t$ is observable but $x_t$ is not. Assuming stationarity, explain how you would estimate $\theta$ by maximum likelihood.}
    
    \vspace{0.4em}
    
    \textbf{Solution.} Our model is a \textbf{linear Gaussian state-space}:
    \begin{itemize}
        \item \textbf{State equation}: \quad $x_t = \rho x_{t-1} + \sigma_\varepsilon \varepsilon_t$
        \item \textbf{Observation equation}: \quad $g_t = x_t + \sigma_\eta \eta_t$
    \end{itemize}
    
    \textbf{Mapping to review notation} ($y_t = g_t$, $s_t = x_t$, $\psi = \theta$):
    \begin{itemize}
        \item Observation: $b_t = 0$, $Z_t = 1$, $H_t = \sigma_\eta^2$
        \item Transition: $A_t = \rho$, $B_t = \sigma_\varepsilon$, $Q_t = 1$
    \end{itemize}
    
    \textbf{Estimation strategy}:
    \begin{enumerate}
        \item Run the \textbf{Kalman filter} to get $\hat{x}_{t|t}$ and prediction errors.
        \item Evaluate the \textbf{likelihood} via prediction error decomposition.
        \item \textbf{Maximize} $\ell_T(\theta)$ numerically over $\theta = (\rho, \sigma_\varepsilon, \sigma_\eta)$.
    \end{enumerate}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 1(c): Kalman Filter Recursions}
    \small
    \textbf{Initialization}: $\hat{x}_{0|0} = 0$, $P_{0|0} = \frac{\sigma_\varepsilon^2}{1-\rho^2}$ (unconditional variance).
    
    \textbf{For $t = 1, \ldots, T$}:
    
    \textit{Prediction step}:
    \begin{align*}
        \hat{x}_{t|t-1} &= \rho \hat{x}_{t-1|t-1}, \quad
        P_{t|t-1} = \rho^2 P_{t-1|t-1} + \sigma_\varepsilon^2
    \end{align*}
    
    \textbf{Update step} (upon observing $g_t$):
    \begin{itemize}
        \item[-] \textbf{Prediction error}: \quad $v_t = g_t - \hat{x}_{t|t-1}$
        \item[-] \textbf{Prediction error variance}: \quad $F_t = P_{t|t-1} + \sigma_\eta^2$ \quad (innovation variance)
        \item[-] \textbf{Kalman gain}: \quad $K_t = P_{t|t-1} / F_t$
        \item[-] \textbf{Updated state estimate}: \quad $\hat{x}_{t|t} = \hat{x}_{t|t-1} + K_t v_t$
        \item[-] \textbf{Updated state variance}: \quad $P_{t|t} = (1 - K_t) P_{t|t-1}$
    \end{itemize}
    
    \textbf{Log-likelihood}: $\ell_T(\theta) = -\frac{1}{2}\sum_{t=1}^T \left[\log F_t + \frac{v_t^2}{F_t}\right]$
\end{frame}

% ============================================================================
% PROBLEM 2: BEVERIDGE-NELSON
% ============================================================================
\begin{frame}[plain]
    \begin{center}
        {\Large \textbf{Problem 2: Beveridge--Nelson Trend}}\\[1em]
        {\large Stochastic Trend Decomposition via VAR}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 2: Setup}
    \textbf{Beveridge--Nelson (1981) trend}:
    \[
    \tau_t = x_t + \E_t \sum_{j=1}^{\infty}(\Delta x_{t+j} - \mu_x)
    \]
    
    \textbf{Interpretation}: The trend $\tau_t$ is the level to which $x_t$ is expected to converge after transient dynamics die out.
    
    \vspace{0.5em}
    
    \textbf{Setup}: Let $y_t = (\Delta x_t, w_t')'$ where:
    \begin{itemize}
        \item $\Delta x_t$ is the growth of the series of interest
        \item $w_t$ contains variables that help predict $\Delta x_t$
    \end{itemize}
    
    \vspace{0.3em}
    
    Assume $y_t$ is covariance stationary with mean zero and follows a VAR(2):
    \[
    y_t = \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + u_t, \quad u_t \sim \mathcal{N}(0, \Sigma_u)
    \]
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 2(a): Variable Selection}
    \small
    \textbf{Question}: \textit{Explain how you choose variables to include in $w_t$.}
    
    \begin{block}{Solution}
        Include variables that \textbf{Granger-cause} $\Delta x_t$.
        
        \begin{itemize}
            \item A variable $w_t$ Granger-causes $\Delta x_t$ if past values of $w$ help predict $\Delta x_t$ beyond what past values of $\Delta x_t$ alone provide.
            \item Formally: $w_{t-j}$ appears with nonzero coefficient in the equation for $\Delta x_t$.
        \end{itemize}
    \end{block}
    
    \textbf{Practical approach}:
    \begin{itemize}
        \item Include leading indicators, related macro variables
        \item Test for Granger causality; drop variables that don't help
        \item Balance predictive power vs.\ parameter proliferation
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 2(b): MLE for VAR(2)}
    \small
    \textbf{Question}: \textit{How would you estimate the VAR parameters by MLE?}
    

    \vspace{0.5em}
    \textbf{Solution}: Conditional log-likelihood (given $y_0, y_{-1}$):
    \[
    \ell_T(\Phi_1, \Phi_2, \Sigma_u) = -\frac{T}{2}\log|\Sigma_u| - \frac{1}{2}\sum_{t=1}^T u_t' \Sigma_u^{-1} u_t
    \]
    where $u_t = y_t - \Phi_1 y_{t-1} - \Phi_2 y_{t-2}$.
    
    \begin{block}{MLE = OLS Equation-by-Equation}
        Under Gaussianity, the MLE for $(\Phi_1, \Phi_2)$ is obtained by OLS on each equation:
        \[
        \hat{\Phi} = \left(\sum_{t=1}^T y_t Z_{t-1}'\right)\left(\sum_{t=1}^T Z_{t-1} Z_{t-1}'\right)^{-1}
        \]
        where $Z_{t-1} = (y_{t-1}', y_{t-2}')'$ and $\hat{\Phi} = [\hat{\Phi}_1, \hat{\Phi}_2]$.
    \end{block}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 2(c): Trend Formula --- Companion Form}
    \small
    \textbf{Question}: \textit{Derive an expression for the trend $\tau_t$ in terms of the VAR parameters and current and lagged values of $y_t$.}
    
    \vspace{0.3em}
    
    \textbf{Step 1.} Write VAR(2) in companion form VAR(1).
    \begin{itemize}
        \item[-] Stack $Y_t = (y_t', y_{t-1}')'$. Then:
    \end{itemize}
    \[
    Y_t = F Y_{t-1} + v_t, \quad F = \begin{pmatrix} \Phi_1 & \Phi_2 \\ I & 0 \end{pmatrix}, \quad v_t = \begin{pmatrix} u_t \\ 0 \end{pmatrix}
    \]
    
    \textbf{Step 2.} Multi-step forecast.
    \begin{itemize}
        \item[-] For stationary VAR(1): $\E_t[Y_{t+h}] = F^h Y_t$.
    \end{itemize}
    
    \textbf{Step 3.} Extract $\Delta x_{t+h}$ forecast.
    \begin{itemize}
        \item[-] Let $e_1 = (1, 0, \ldots, 0)'$ select $\Delta x_t$ from $Y_t$.
        \item[-] Then: $\E_t[\Delta x_{t+h}] = e_1' F^h Y_t$.
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 2(c): Trend Formula --- Final Expression}
    \small
    \textbf{BN trend} (definition): $\tau_t = x_t + \sum_{h=1}^{\infty} \E_t[\Delta x_{t+h}]$.
    \begin{itemize}
        \item[-] From Step 3: $\E_t[\Delta x_{t+h}] = e_1' F^h Y_t$, so
    \end{itemize}
    \[
    \tau_t = x_t + \sum_{h=1}^{\infty} e_1' F^h Y_t
    \]
    
    \textbf{Geometric series} (stable $F$, eigenvalues inside unit circle):
    \begin{itemize}
        \item[-] $\sum_{h=1}^{\infty} F^h = F(I - F)^{-1}$.
    \end{itemize}
    
    \begin{alertblock}{Beveridge--Nelson Trend Formula}
        \[
        \boxed{\tau_t = x_t + e_1' F(I-F)^{-1} Y_t}
        \]
        where $Y_t = (y_t', y_{t-1}')'$.
    \end{alertblock}
    
    \textbf{Cycle}: $c_t = x_t - \tau_t = -e_1' F(I-F)^{-1} Y_t$.
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 2(d): Asymptotic Distribution}
    \small
    \textbf{Question}: \textit{How would you approximate the asymptotic distribution of the coefficients mapping current and lagged values of $y_t$ into $\tau_t$?}
    
    \vspace{0.5em}
    
    \textbf{Solution}: Let $\psi = \text{vec}(\Phi_1, \Phi_2)$ be the VAR parameters and $h(\psi) = e_1'F(I-F)^{-1}$ be the coefficient mapping $Y_t$ to the trend adjustment.
    
    \begin{block}{Delta Method}
        Since $\sqrt{T}(\hat{\psi} - \psi_0) \xrightarrow{d} \mathcal{N}(0, V_\psi)$, we have:
        \[
        \sqrt{T}(h(\hat{\psi}) - h(\psi_0)) \xrightarrow{d} \mathcal{N}\left(0, \frac{\partial h}{\partial \psi'} V_\psi \frac{\partial h'}{\partial \psi}\right)
        \]
    \end{block}
    
    \textbf{In practice}:
    \begin{itemize}
        \item Compute $\hat{V}_\psi$ from VAR estimation (standard errors)
        \item Compute Jacobian $\partial h/\partial \psi'$ numerically or analytically
        \item Or use bootstrap for small-sample inference
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 2(e): Random Walk Property}
    \textbf{Question}: \textit{Prove that $\tau_t$ is a random walk (with drift if $\mu_{\Delta x} \neq 0$).}
    
    \small
    \textbf{Proof}: From $\tau_t = x_t + e_1' F(I-F)^{-1} Y_t$, take first differences:
    \begin{align*}
        \Delta \tau_t &= \Delta x_t + e_1' F(I-F)^{-1} \Delta Y_t \\
        &= \Delta x_t + e_1' F(I-F)^{-1} (Y_t - Y_{t-1})
    \end{align*}
    
    Using $Y_t = F Y_{t-1} + v_t$, we get $\Delta Y_t = (F-I)Y_{t-1} + v_t$:
    \begin{align*}
        \Delta \tau_t &= e_1' v_t + e_1' F(I-F)^{-1}[(F-I)Y_{t-1} + v_t] \\
        &= e_1' v_t - e_1' F Y_{t-1} + e_1' F(I-F)^{-1} v_t \\
        &= e_1'[I + F(I-F)^{-1}] v_t = e_1' (I-F)^{-1} v_t
    \end{align*}
    
    \begin{alertblock}{Result}
        $\Delta \tau_t = e_1' (I-F)^{-1} v_t$ is a linear combination of $v_t$, which is i.i.d.
        
        $\Rightarrow$ $\tau_t$ is a \textbf{random walk} (with drift if $\mu_{\Delta x} \neq 0$).
    \end{alertblock}
\end{frame}

% ============================================================================
% PROBLEM 3: CLARIDA-GALI-GERTLER
% ============================================================================
\begin{frame}[plain]
    \begin{center}
        {\Large \textbf{Problem 3: Clarida--Gal\'{\i}--Gertler (1999)}}\\[1em]
        {\large NK Model, Policy Regimes, and the Lucas Critique}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3: U.S.\ Monetary Policy History --- Context}
    \small
    \textbf{Summary of the period} (relevant for Burns vs.\ Volcker and the Lucas critique):
    \begin{itemize}
        \item \textbf{1960s--70s}: Great Inflation; Fed often \textbf{accommodative} (raised rates less than inflation); Arthur Burns chair (1970--78).
        \item \textbf{1979--87 (Volcker)}: Sharp shift to \textbf{anti-inflation} policy; disinflation at cost of recession; Taylor-rule interpretation: $\phi_\pi$ increased.
        \item \textbf{Pre-Volcker vs.\ Volcker}: Clarida, Gal\'{\i}, Gertler (1999) document that estimated Fed reaction to inflation was \textbf{weaker} pre-Volcker and \textbf{stronger} after; forward-looking Taylor rules fit the regime change.
    \end{itemize}
    \vspace{0.4em}
    \textbf{Recommendation}: For a full narrative of U.S.\ monetary and fiscal policy over 1961--2021 (eight Fed chairs, stagflation to COVID), see Blinder (2022), \textit{A Monetary and Fiscal History of the United States, 1961--2021}, Princeton.
    \vspace{0.3em}
    \textbf{Sources}: Clarida, Gal\'{\i}, Gertler (1999), \textit{J.\ Econ.\ Lit.}; Blinder (2022), Princeton University Press.
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3: The NK Model}
    \begin{columns}
    \begin{column}{0.55\textwidth}
    \textbf{3-equation NK model}:
    \begin{align*}
        \text{(IS)} \quad x_t &= \E_t x_{t+1} - \frac{1}{\sigma}(i_t - \E_t\pi_{t+1} - r^n) \\[0.3em]
        \text{(NKPC)} \quad \pi_t &= \beta \E_t\pi_{t+1} + \kappa x_t + u_t \\[0.3em]
        \text{(Taylor)} \quad i_t &= r^n + \phi_\pi \pi_t
    \end{align*}
    
    \vspace{0.3em}
    
    \textbf{Assumptions}:
    \begin{enumerate}
        \item[(A1)] $r_t^n = r^n$ constant
        \item[(A2)] $\E_t x_{t+1} = 0$
        \item[(A3)] $\E_t \pi_{t+1} = \pi_{t-1}$
        \item[(A4)] $u_t \sim \text{i.i.d.}(0, \sigma_u^2)$
    \end{enumerate}
    \end{column}
    \begin{column}{0.44\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{figures/ffs_inf.png}
    \end{center}
    {\small Federal funds rate and inflation (CGG)}
    \end{column}
    \end{columns}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(a): Deriving the AR(1) for Inflation --- Step 1}
    \small
    \textbf{Question}: Eliminate $x_t$ and $i_t$; show $\pi_t = a\pi_{t-1}+\varepsilon_t$. Give $a$, $\varepsilon_t$; sign of $\partial a/\partial\phi_\pi$ and interpret.
    \begin{itemize}
        \item \textbf{Step 1}: From IS with (A1)--(A3): $x_t = -\frac{1}{\sigma}(i_t - \pi_{t-1} - r^n)$
        \item \textbf{Step 2}: Substitute Taylor $i_t = r^n + \phi_\pi \pi_t$:
        \[
        x_t = -\frac{1}{\sigma}(\phi_\pi \pi_t - \pi_{t-1})
        \]
        \item Hence: $\boxed{x_t = \frac{1}{\sigma}\pi_{t-1} - \frac{\phi_\pi}{\sigma}\pi_t}$
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(a): Deriving the AR(1) for Inflation --- Step 2}
    \textbf{Step 3}: Substitute $x_t$ into NKPC.
    \begin{itemize}
        \item (A3) $\Rightarrow$ $\E_t\pi_{t+1} = \pi_{t-1}$: NKPC is $\pi_t = \beta \pi_{t-1} + \kappa x_t + u_t$
        \item Substitute $x_t = \frac{1}{\sigma}\pi_{t-1} - \frac{\phi_\pi}{\sigma}\pi_t$:
        \[
        \pi_t = \beta \pi_{t-1} + \kappa\left(\frac{1}{\sigma}\pi_{t-1} - \frac{\phi_\pi}{\sigma}\pi_t\right) + u_t
        \]
        \item Collect $\pi_t$ and $\pi_{t-1}$:
        \[
        \pi_t \left(1 + \frac{\kappa\phi_\pi}{\sigma}\right) = \left(\beta + \frac{\kappa}{\sigma}\right)\pi_{t-1} + u_t
        \]
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(a): AR(1) Result}
    \begin{alertblock}{Reduced-Form AR(1) for Inflation}
        $\pi_t = a \cdot \pi_{t-1} + \varepsilon_t$ with
        $a = \frac{\beta\sigma + \kappa}{\sigma + \kappa\phi_\pi}$,
        $\varepsilon_t = \dfrac{\sigma u_t}{\sigma + \kappa\phi_\pi}$
    \end{alertblock}
    \begin{itemize}
        \item \textbf{Sign}: $\dfrac{\partial a}{\partial \phi_\pi} = \dfrac{-\kappa(\beta\sigma + \kappa)/\sigma}{(\sigma + \kappa\phi_\pi)^2} < 0$
        \item \textbf{Interpretation}: More aggressive policy ($\uparrow \phi_\pi$) $\Rightarrow$ lower persistence ($\downarrow a$)
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(b): Burns vs.\ Volcker Hypothesis}
    \small
    \textbf{Question}: Translate ``policy changed from Burns to Volcker'' into $H_0$ and $H_1$ in terms of $a_1$, $a_2$.
    \begin{itemize}
        \item Break at $T_b$ (1979:Q3): $\phi_\pi = \phi_{\pi,1}$ for $t \leq T_b$, $\phi_\pi = \phi_{\pi,2}$ for $t > T_b$
        \item $\partial a/\partial\phi_\pi < 0$ $\Rightarrow$ Burns $\to$ $a_1$, Volcker $\to$ $a_2$
        \item ``Volcker more aggressive'' $\Rightarrow$ $\phi_{\pi,2} > \phi_{\pi,1}$ $\Rightarrow$ $a_2 < a_1$
    \end{itemize}
    \begin{alertblock}{Hypotheses}
        $H_0: a_1 = a_2$ (no change) \quad vs. \quad $H_1: a_1 > a_2$ (Volcker more aggressive)
    \end{alertblock}
\end{frame}


% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(c): Conditional Log-Likelihood}
    \small
    \textbf{Question}: (i) Conditional log-likelihood $\ell_n(\theta)$ given $\pi_0$, $\theta=(c,a_1,a_2,\sigma^2)$. (ii) LR test for $H_0:a_1=a_2$ and its asymptotic distribution.
    \begin{center}
        $t \leq T_b$:\quad $\pi_t = c + a_1\pi_{t-1} + \varepsilon_t$\\[0.5em]
        $t > T_b$:\quad $\pi_t = c + a_2\pi_{t-1} + \varepsilon_t$\\[0.3em]
        $\varepsilon_t \mid \mathcal{F}_{t-1} \sim \mathcal{N}(0, \sigma^2)$
    \end{center}
    \begin{block}{Conditional Log-Likelihood (given $\pi_0$)}
        The sum \textbf{splits at the break}: use $a_1$ for $t \leq T_b$, $a_2$ for $t > T_b$:
        \[
            \ell_n(\theta) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\left[\sum_{t=1}^{T_b}(\pi_t - c - a_1\pi_{t-1})^2 + \sum_{t=T_b+1}^{n}(\pi_t - c - a_2\pi_{t-1})^2\right]
        \]
    \end{block}
\end{frame}

% ----------------------------------------------------------------------------

% ----------------------------------------------------------------------------
\begin{frame}{Quick review: Asymptotic tests}
    \small
    \textbf{Where do we use them?}
    \begin{itemize}
        \item As $n \to \infty$, asymptotic approximation works better; we can derive tests even in complicated problems where no optimal (finite-sample) test exists.
    \end{itemize}
    \vspace{0.4em}
    \textbf{Trinity of large-sample tests}
    \begin{itemize}
        \item \textbf{LR}: distance between log-likelihoods (restricted vs.\ unrestricted)
        \item \textbf{Wald}: distance between estimators ($\hat{\theta}$ vs.\ value under $H_0$)
        \item \textbf{Score (LM)}: distance to zero score (score at restricted MLE)
        \begin{center}
            $S(\theta) = \nabla_\theta \ell_n(\theta) = \bigl( \frac{\partial \ell_n}{\partial \theta_1}, \ldots, \frac{\partial \ell_n}{\partial \theta_k} \bigr)'$
        \end{center}
    \end{itemize}
    \vspace{0.4em}
    \textbf{Differences} (what to estimate)
    \begin{itemize}
        \item \textbf{LR}: estimate both restricted and unrestricted models
        \item \textbf{Wald}: estimate only unrestricted model (for simple null)
        \item \textbf{LM}: estimate only restricted model
    \end{itemize}
    % \vspace{0.3em}
    % \textbf{Recall}: Score $= S(\theta) = \nabla_\theta \ell_n(\theta) = \bigl( \frac{\partial \ell_n}{\partial \theta_1}, \ldots, \frac{\partial \ell_n}{\partial \theta_k} \bigr)'$
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(c): LR Test for $H_0: a_1 = a_2$}
    \small
    \begin{itemize}
        \item \textbf{Unrestricted}: MLE $\hat{\theta}_U = (\hat{c}, \hat{a}_1, \hat{a}_2, \hat{\sigma}^2)$
        \[
        \text{RSS}_U = \sum_{t=1}^{T_b}(\pi_t - \hat{c} - \hat{a}_1\pi_{t-1})^2 + \sum_{t=T_b+1}^{n}(\pi_t - \hat{c} - \hat{a}_2\pi_{t-1})^2
        \]
        \item \textbf{Restricted}: impose $a_1 = a_2 = a$, MLE $\tilde{\theta}_R = (\tilde{c}, \tilde{a}, \tilde{\sigma}^2)$
        \[
        \text{RSS}_R = \sum_{t=1}^{n}(\pi_t - \tilde{c} - \tilde{a}\pi_{t-1})^2
        \]
        \item \textbf{MLE variance}: $\hat{\sigma}^2 = \text{RSS}/n$ for each model
        \item \textbf{Statistic}: $LR = 2[\ell_n(\hat{\theta}_U) - \ell_n(\hat{\theta}_R)]$
        \item \textbf{Derivation}: plug $\hat{\sigma}^2 = \text{RSS}/n$ into $\ell_n$, simplify $\Rightarrow$
        \[
        LR = n\log\!\left(\frac{\text{RSS}_R}{\text{RSS}_U}\right) \geq 0
        \]
        \item \textbf{Under $H_0$}: $LR \xrightarrow{d} \chi^2(1)$ (one restriction: $a_1 - a_2 = 0$){\footnotesize \hyperlink{appendix:lr}{\beamergotobutton{General LR theory}}}
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(d): Wald Test for $H_0: a_1 = a_2$}
    \small
    \textbf{Wald}: uses \textbf{unrestricted} MLE $\hat{\theta}_U = (\hat{c}, \hat{a}_1, \hat{a}_2, \hat{\sigma}^2)$
    \begin{itemize}
        \item \textbf{Restriction}: $r(\theta) = a_1 - a_2 = 0$
        \item \textbf{Statistic}: $W = \dfrac{(\hat{a}_1 - \hat{a}_2)^2}{\widehat{\Var}(\hat{a}_1 - \hat{a}_2)}$
        \item \textbf{Denominator}: $\widehat{\Var}(\hat{a}_1 - \hat{a}_2)$ comes from the \textbf{estimated asymptotic covariance matrix} of $\hat{\theta}_U$ (e.g.\ inverse Hessian or OLS $(X'X)^{-1}$ block).
        \item \textbf{Under $H_0$}: $W \xrightarrow{d} \chi^2(1)$
    \end{itemize}
    \vspace{0.2em}
    {\footnotesize \hyperlink{appendix:wald}{\beamergotobutton{See Appendix: General Wald theory}}}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(d): LM (Score) Test for $H_0: a_1 = a_2$}
    \small
    \textbf{Restricted} MLE $\tilde{\theta}_R = (\tilde{c}, \tilde{a}, \tilde{\sigma}^2)$; $\tilde{\varepsilon}_t = \pi_t - \tilde{c} - \tilde{a}\pi_{t-1}$.
    \begin{itemize}
        \item \textbf{Joint score} (sum over $t=1,\ldots,n$; same $\tilde{\varepsilon}_t$ under restricted model):
        $S_{a_1} = \frac{1}{\tilde{\sigma}^2}\sum_{t=1}^{T_b} \tilde{\varepsilon}_t \pi_{t-1}$, \quad
        $S_{a_2} = \frac{1}{\tilde{\sigma}^2}\sum_{t=T_b+1}^{n} \tilde{\varepsilon}_t \pi_{t-1}$.
        Restricted FOC $\Rightarrow$ $S_{a_1} + S_{a_2} = 0$.
        \item \textbf{Hypothesis only on $a$'s} $\Rightarrow$ use score $S_a = (S_{a_1}, S_{a_2})'$ and the \textbf{$(a_1,a_2)$ block} of the Fisher information.
        \item \textbf{Fisher information for $(a_1, a_2)$}: $\frac{\partial^2\ell}{\partial a_1\partial a_2} = 0$ (disjoint sums) $\Rightarrow$
        \[
        \mathcal{I}_{aa} = \frac{1}{\tilde{\sigma}^2}\begin{bmatrix} q_1 & 0 \\ 0 & q_2 \end{bmatrix}, \quad q_1 = \sum_{t=1}^{T_b}\pi_{t-1}^2,\; q_2 = \sum_{t=T_b+1}^{n}\pi_{t-1}^2
        \]
        (When $c$ is also estimated, use the $(a_1,a_2)$ block of the \emph{inverse} of the full $(c,a_1,a_2)$ information.)
        \item \textbf{LM}: $LM = S_a' \, \mathcal{I}_{aa}^{-1} \, S_a$ (or with the full-inverse $(a_1,a_2)$ block). Under $H_0$: $LM \xrightarrow{d} \chi^2(1)$.
    \end{itemize}
    \vspace{0.2em}
    {\footnotesize \hyperlink{appendix:lm}{\beamergotobutton{See Appendix: General LM theory}}}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(e): Interpretation}
    \small
    \textbf{Question}: If $\hat{a}_2 < \hat{a}_1$ and we reject $H_0$, interpret $\phi_{\pi,2}$ vs.\ $\phi_{\pi,1}$.
    \begin{itemize}
        \item Persistence \textbf{fell} after $T_b$
        \item $\partial a/\partial\phi_\pi < 0$ $\Rightarrow$ $\phi_{\pi,2} > \phi_{\pi,1}$
        \item \textbf{Volcker more aggressive} than Burns
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 3(f): Lucas Critique}
    \small
    \textbf{Question}: Why does an AR(1) estimated under Burns fail to forecast under Volcker? Structural vs.\ reduced-form.
    \begin{alertblock}{Why AR(1) from Burns Fails Under Volcker}
        \begin{itemize}
            \item \textbf{Structural}: $\phi_\pi$ (policy), $\kappa, \beta, \sigma$ (deep)
            \item \textbf{Reduced-form}: $a = \frac{\beta\sigma + \kappa}{\sigma + \kappa\phi_\pi}$ (depends on policy)
            \item Burns-era AR(1) uses $a_1 = a(\phi_{\pi,1})$; after switch, true $a = a_2 \neq a_1$
            \item $\Rightarrow$ \textbf{Forecasts based on $a_1$ systematically wrong}
        \end{itemize}
    \end{alertblock}
\end{frame}

% ============================================================================
% PROBLEM 4: KALMAN WITH CORRELATED INNOVATIONS
% ============================================================================
\begin{frame}[plain]
    \begin{center}
        {\Large \textbf{Problem 4: Kalman Filter}}\\[1em]
        {\large With Correlated State and Measurement Innovations}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 4: Question}
    \textbf{Question}: Derive the Kalman filter for this model.
    \begin{align*}
        S_t &= A S_{t-1} + B \varepsilon_{1t} & &\text{(State)} \\[0.5em]
        X_t &= C S_t + D \varepsilon_{2t}     & &\text{(Measurement)}
    \end{align*}
    Shocks are normally distributed:
    \[
    \begin{pmatrix} \varepsilon_{1t} \\ \varepsilon_{2t} \end{pmatrix} \sim \mathcal{N}\left(\mathbf{0},\, \begin{pmatrix} Q & F \\ F' & R \end{pmatrix}\right)
    \]
    \textbf{Innovation}: $F \neq 0$ \quad $\Rightarrow$ \quad measurement shock correlated with structural (state) shock.
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 4: Definitions}
    \small
    \begin{itemize}
        \item \textbf{Predicted state}: \quad $\hat{S}_{t|t-1} = \E[S_t \mid X_{t-1}, \ldots, X_1]$
        \item \textbf{Prediction MSE}: \quad $P_{t|t-1} = \Var(S_t \mid X_{t-1}, \ldots, X_1)$
        \item \textbf{Innovation (prediction error)}: \quad $\nu_t = X_t - C \hat{S}_{t|t-1}$
        \item \textbf{Innovation variance}: \quad $\Sigma_\nu = \Var(\nu_t)$
        \item \textbf{Kalman gain}: \quad $K_t$ (weight on $\nu_t$ in the update)
        \item \textbf{Updated state}: \quad $\hat{S}_{t|t} = \E[S_t \mid X_t, \ldots, X_1]$, \quad $P_{t|t} = \Var(S_t \mid X_t, \ldots, X_1)$
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 4: Modified Kalman Filter --- Algorithm}
    \small
    \textbf{Initialization}: $\hat{S}_{0|0}$, $P_{0|0}$ (e.g.\ unconditional or prior).
    \vspace{0.6em}
    \textbf{For $t = 1, \ldots, T$}:
    \vspace{0.3em}
    \textit{Prediction step} (same as standard; no $F$):
    \begin{align*}
        \hat{S}_{t|t-1} &= A \hat{S}_{t-1|t-1}, &
        P_{t|t-1} &= A P_{t-1|t-1} A' + B Q B'
    \end{align*}
    \vspace{0.3em}
    \textit{Update step} (upon observing $X_t$):
    \begin{enumerate}
        \item \textbf{Prediction error (innovation)}: \quad $\nu_t = X_t - C \hat{S}_{t|t-1}$
        \item \textbf{Innovation variance} (modified; cross terms in $F$): {\footnotesize \hyperlink{appendix:innovation-variance}{\beamergotobutton{Derivation}}}
        \[
        \Sigma_\nu = C P_{t|t-1} C' + D R D' + C B F D' + D F' B' C'
        \]
        \item \textbf{Kalman gain} (modified): \quad $K_t = (P_{t|t-1} C' + B F D')\, \Sigma_\nu^{-1}$
        \item \textbf{Updated state estimate}: \quad $\hat{S}_{t|t} = \hat{S}_{t|t-1} + K_t \nu_t$
        \item \textbf{Updated state MSE}: \quad $P_{t|t} = P_{t|t-1} - K_t \Sigma_\nu K_t'$
    \end{enumerate}
\end{frame}

% ============================================================================
% PROBLEM 5: ARMA TO STATE SPACE
% ============================================================================
\begin{frame}[plain]
    \begin{center}
        {\Large \textbf{Problem 5: ARMA(2,2) to State Space}}\\[1em]
        {\large Converting ARMA to State-Space Form}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 5: Setup}
    \small
    \textbf{Question}: Put ARMA(2,2) into state-space form and verify.
    \begin{itemize}
        \item \textbf{ARMA(2,2)}: $(1 - \rho_1 L - \rho_2 L^2) y_t = (1 + \theta_1 L + \theta_2 L^2) \varepsilon_t$, \quad $\varepsilon_t \sim \text{i.i.d.\ } \mathcal{N}(0, \sigma^2)$
        \item \textbf{Expanded}: $y_t = \rho_1 y_{t-1} + \rho_2 y_{t-2} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2}$
        \item \textbf{Goal}: State-space $S_t = A S_{t-1} + B u_t$, \quad $y_t = C S_t$
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 5: State-Space Representation}
    \small
    \begin{itemize}
        \item \textbf{State}: $S_t = (y_t, y_{t-1}, \varepsilon_t, \varepsilon_{t-1})'$
        \item \textbf{State equation} $S_t = A S_{t-1} + B \varepsilon_t$:
    \end{itemize}
    \[
    \begin{pmatrix} y_t \\ y_{t-1} \\ \varepsilon_t \\ \varepsilon_{t-1} \end{pmatrix}
    =
    \begin{pmatrix} \rho_1 & \rho_2 & \theta_1 & \theta_2 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{pmatrix}
    \begin{pmatrix} y_{t-1} \\ y_{t-2} \\ \varepsilon_{t-1} \\ \varepsilon_{t-2} \end{pmatrix}
    +
    \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix} \varepsilon_t
    \]
    \begin{itemize}
        \item \textbf{Measurement}: $y_t = C S_t$, \quad $C = (1, 0, 0, 0)$
    \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Problem 5: Verification}
    \small
    \begin{itemize}
        \item \textbf{Verify}: First row of state equation gives
        \[
        y_t = \rho_1 y_{t-1} + \rho_2 y_{t-2} + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \varepsilon_t
        \]
        \item Matches ARMA(2,2): $y_t = \rho_1 y_{t-1} + \rho_2 y_{t-2} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2}$ \checkmark
    \end{itemize}
    \begin{block}{Summary}
        State $S_t = (y_t, y_{t-1}, \varepsilon_t, \varepsilon_{t-1})'$; \quad $A$ (4$\times$4 above), $B = (1, 0, 1, 0)'$; \quad $C = (1, 0, 0, 0)$, $D = 0$
    \end{block}
\end{frame}

% ============================================================================
% SUMMARY
% ============================================================================
\begin{frame}{Summary}
    \small
    \textbf{Problem 1 (Bansal--Yaron)}:
    \begin{itemize}
        \item VAR(1) representation; stationarity requires $|\rho| < 1$
        \item Kalman filter for MLE when state is latent
    \end{itemize}
    
    \textbf{Problem 2 (Beveridge--Nelson)}:
    \begin{itemize}
        \item Trend formula: $\tau_t = x_t + e_1' F(I-F)^{-1} Y_t$; BN trend is a random walk
    \end{itemize}
    
    \textbf{Problem 3 (CGG)}:
    \begin{itemize}
        \item AR(1) persistence $a = \frac{\beta\sigma + \kappa}{\sigma + \kappa\phi_\pi}$; $\partial a/\partial\phi_\pi < 0$
        \item LR, Wald, LM tests for structural break; Lucas critique
    \end{itemize}
    
    \textbf{Problem 4}: Modified Kalman gain when innovations correlated
    
    \textbf{Problem 5}: ARMA(2,2) $\rightarrow$ 4-dimensional state space
\end{frame}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\begin{frame}[plain]
    \begin{center}
        {\Large \textbf{Appendix}}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: General LR Test Theory}
    \small \label{appendix:lr}
    \textbf{LR test} (one of the most useful methods for complicated problems):
    \begin{itemize}
        \item Reject $H_0$ if $x \in \left\{ x : \lambda(x) = \dfrac{\sup_{\theta \in \Theta_0} \ell(\theta|x)}{\sup_{\theta \in \Theta} \ell(\theta|x)} \leq c \right\}$
        \item Even if we cannot obtain the suprema analytically, we can compute them numerically
        \item Choose $c$ so that $\sup_{\theta \in \Theta_0} \mathbb{P}_\theta(\lambda(X) \leq c) \leq \alpha$ ($\alpha$ is the the level of the test)
    \end{itemize}
    \vspace{0.3em}
    \textbf{Asymptotic distribution} (under regularity conditions):
    \begin{itemize}
        \item Under $H_0: \theta = \theta_0$: \quad $-2\ln\lambda(X) \xrightarrow{d} \chi^2_q$ \quad ($q$ = \# restrictions)
        \item \textbf{Proof sketch}: Taylor expand $\ln\ell(\theta|x)$ around $\hat{\theta}$:
        \[
        -2\ln\lambda(x) = 2[\ln\ell(\hat{\theta}|x) - \ln\ell(\theta_0|x)] \approx -\ln\ell''(\theta_0|x)(\theta_0 - \hat{\theta})^2
        \]
        \item Use $-\frac{1}{n}\ln\ell''(\hat{\theta}|x) \xrightarrow{p} \mathcal{I}(\theta_0)$ and $\sqrt{n}(\hat{\theta} - \theta_0) \xrightarrow{d} N(0, \mathcal{I}(\theta_0)^{-1})$
    \end{itemize}
    \vspace{0.3em}
    {\footnotesize \textbf{Caveat}: The above is for i.i.d.\ samples. With time series, replace with appropriate regularity conditions (e.g.\ ergodicity, mixing) for dependent data.}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: LR Derivation for Gaussian Regression}
    \small
    \textbf{Setup}: Gaussian log-likelihood
    \[
    \ell_n(\theta) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\text{RSS}(\theta)
    \]
    \begin{itemize}
        \item \textbf{MLE for $\sigma^2$}: $\hat{\sigma}^2 = \text{RSS}/n$
        \[
        \ell_n(\hat{\theta}) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\!\left(\frac{\text{RSS}}{n}\right) - \frac{n}{2}
        \]
        \item \textbf{LR statistic}:
        \begin{align*}
        LR &= 2[\ell_n(\hat{\theta}_U) - \ell_n(\hat{\theta}_R)]
           = 2\left[-\frac{n}{2}\log\!\left(\frac{\text{RSS}_U}{n}\right) + \frac{n}{2}\log\!\left(\frac{\text{RSS}_R}{n}\right)\right]\\
           &= n\log\!\left(\frac{\text{RSS}_R}{\text{RSS}_U}\right)
        \end{align*}
    \end{itemize}
    \begin{alertblock}{Result}
        $LR = n\log(\text{RSS}_R/\text{RSS}_U) \xrightarrow{d} \chi^2(q)$ under $H_0$ ($q$ = \# restrictions)
    \end{alertblock}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: General Wald Test Theory}
    \small \label{appendix:wald}
    \textbf{Wald test}: tests $H_0: r(\theta) = 0$ using \textbf{unrestricted} MLE $\hat{\theta}$
    \begin{itemize}
        \item \textbf{Idea}: If $H_0$ is true, $r(\hat{\theta})$ should be close to zero.
        \item \textbf{Statistic}:
        \[
        W = r(\hat{\theta})' \left[ R(\hat{\theta}) \, \widehat{\Var}(\hat{\theta}) \, R(\hat{\theta})' \right]^{-1} r(\hat{\theta})
        \]
        where $R(\theta) = \partial r / \partial \theta'$ is the Jacobian of restrictions.
        \item \textbf{Scalar case} ($q=1$ restriction): $W = \dfrac{[r(\hat{\theta})]^2}{\widehat{\Var}(r(\hat{\theta}))}$
        \item \textbf{Under $H_0$}: $W \xrightarrow{d} \chi^2(q)$ ($q$ = \# restrictions)
    \end{itemize}
    \vspace{0.3em}
    \textbf{Advantage}: Only need to estimate the unrestricted model.\\
    {\footnotesize \textbf{Caveat}: For time series, use HAC standard errors if innovations are serially correlated.}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: General LM (Score) Test Theory}
    \small \label{appendix:lm}
    \textbf{LM test}: tests $H_0: r(\theta) = 0$ using \textbf{restricted} MLE $\tilde{\theta}$
    \begin{itemize}
        \item \textbf{Idea}: Under $H_0$, the score $S(\theta_0) = \nabla_\theta \ell_n(\theta_0)$ has mean zero. If $S(\tilde{\theta})$ is ``large,'' reject $H_0$.
        \item \textbf{Statistic}:
        \[
        LM = S(\tilde{\theta})' \mathcal{I}(\tilde{\theta})^{-1} S(\tilde{\theta})
        \]
        where $\mathcal{I}(\theta) = -\E[\nabla^2_\theta \ell_n(\theta)]$ is the Fisher information.
        \item \textbf{Under $H_0$}: $LM \xrightarrow{d} \chi^2(q)$ ($q$ = \# restrictions)
    \end{itemize}
    \vspace{0.3em}
    \textbf{Advantage}: Only need to estimate the restricted model (useful when unrestricted is hard to estimate).\\
    {\footnotesize \textbf{Caveat}: For time series, replace Fisher information with appropriate long-run variance if needed.}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: Trinity --- Asymptotic Equivalence}
    \small
    Under regularity conditions and $H_0$:
    \[
    LR, \; W, \; LM \;\xrightarrow{d}\; \chi^2(q)
    \]
    \begin{itemize}
        \item All three are \textbf{asymptotically equivalent} under the null.
        \item In finite samples, they can differ; no universal ranking.
        \item \textbf{Rule of thumb}:
        \begin{itemize}
            \item Use \textbf{LR} if you can estimate both models easily.
            \item Use \textbf{Wald} if unrestricted model is easy, restricted is hard.
            \item Use \textbf{LM} if restricted model is easy, unrestricted is hard.
        \end{itemize}
    \end{itemize}
    \vspace{0.3em}
    {\footnotesize \textbf{Time series caveat}: The i.i.d.\ theory extends to dependent data under ergodicity/mixing conditions; may need HAC variance estimators.}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: Innovation Variance Derivation}
    \small \label{appendix:innovation-variance}
    \textbf{Goal}: Derive $\Sigma_\nu = \Var(\nu_t)$ when $\Cov(\varepsilon_{1t}, \varepsilon_{2t}) = F \neq 0$.
    \vspace{0.5em}
    
    \textbf{Step 1}: Write the innovation in terms of prediction error $\tilde{S}_t = S_t - \hat{S}_{t|t-1}$:
    \[
    \nu_t = X_t - C\hat{S}_{t|t-1} = C(S_t - \hat{S}_{t|t-1}) + D\varepsilon_{2t} = C\tilde{S}_t + D\varepsilon_{2t}
    \]
    
    \textbf{Step 2}: Compute variance (key: $\tilde{S}_t$ involves $\varepsilon_{1t}$, and $\Cov(\varepsilon_{1t}, \varepsilon_{2t}) = F$):
    \begin{align*}
    \Sigma_\nu &= \Var(C\tilde{S}_t + D\varepsilon_{2t}) \\
    &= C \underbrace{\Var(\tilde{S}_t)}_{P_{t|t-1}} C' + D \underbrace{\Var(\varepsilon_{2t})}_{R} D' + C \Cov(\tilde{S}_t, \varepsilon_{2t}) D' + D \Cov(\varepsilon_{2t}, \tilde{S}_t) C'
    \end{align*}
    Since $S_t = AS_{t-1} + B\varepsilon_{1t}$ with $S_{t-1} \perp \varepsilon_{2t}$, and $\hat{S}_{t|t-1} \perp \varepsilon_{2t}$: \quad $\Cov(\tilde{S}_t, \varepsilon_{2t}) = B \Cov(\varepsilon_{1t}, \varepsilon_{2t}) = BF$.
    \[
    \boxed{\Sigma_\nu = C P_{t|t-1} C' + D R D' + C B F D' + D F' B' C'}
    \]
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: Bayesian Updating in Linear Gaussian Models}
    \small
    \textbf{Key fact}: If $(X, Y)$ are jointly normal, then $X \mid Y$ is also normal with:
    \[
    \E[X \mid Y] = \E[X] + \Cov(X, Y)\, \Var(Y)^{-1} (Y - \E[Y])
    \]
    \[
    \Var(X \mid Y) = \Var(X) - \Cov(X, Y)\, \Var(Y)^{-1} \Cov(Y, X)
    \]
    \vspace{0.5em}
    \textbf{Application to Kalman filter}: Let $X = S_t$ (state), $Y = \nu_t$ (innovation). Then:
    \begin{itemize}
        \item $\Cov(S_t, \nu_t) = \Cov(\tilde{S}_t, \nu_t) = P_{t|t-1} C' + B F D'$
        \item $\Var(\nu_t) = \Sigma_\nu$
    \end{itemize}
    \vspace{0.3em}
    The Kalman update is exactly Bayesian updating:
    \begin{align*}
        K_t &= \Cov(\tilde{S}_t, \nu_t)\, \Sigma_\nu^{-1} = (P_{t|t-1} C' + B F D')\, \Sigma_\nu^{-1} \\[0.3em]
        \hat{S}_{t|t} &= \hat{S}_{t|t-1} + K_t \nu_t \quad \text{(conditional mean)} \\[0.3em]
        P_{t|t} &= P_{t|t-1} - K_t \Sigma_\nu K_t' \quad \text{(conditional variance)}
    \end{align*}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}{Appendix: Kalman Gain Derivation}
    \small
    \textbf{Goal}: Derive $K_t = (P_{t|t-1} C' + BFD')\Sigma_\nu^{-1}$.
    \vspace{0.5em}
    
    \textbf{Step 1}: From Bayesian updating, $K_t = \Cov(\tilde{S}_t, \nu_t)\, \Sigma_\nu^{-1}$.
    \vspace{0.3em}
    
    \textbf{Step 2}: Compute $\Cov(\tilde{S}_t, \nu_t)$ where $\nu_t = C\tilde{S}_t + D\varepsilon_{2t}$:
    \begin{align*}
        \Cov(\tilde{S}_t, \nu_t) &= \Cov(\tilde{S}_t, C\tilde{S}_t + D\varepsilon_{2t}) \\
        &= \Cov(\tilde{S}_t, C\tilde{S}_t) + \Cov(\tilde{S}_t, D\varepsilon_{2t}) \\
        &= \Var(\tilde{S}_t) C' + \Cov(\tilde{S}_t, \varepsilon_{2t}) D' \\
        &= P_{t|t-1} C' + BF \cdot D'
    \end{align*}
    (using $\Cov(\tilde{S}_t, \varepsilon_{2t}) = BF$ from the innovation variance derivation)
    \vspace{0.3em}
    
    \textbf{Step 3}: Substitute into Bayesian formula:
    \[
    \boxed{K_t = (P_{t|t-1} C' + BFD')\, \Sigma_\nu^{-1}}
    \]
    \textbf{Note}: When $F = 0$, this reduces to standard Kalman gain $K_t = P_{t|t-1} C' \Sigma_\nu^{-1}$.
\end{frame}

\end{document}
